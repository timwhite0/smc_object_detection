{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "torch.set_default_device(device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from scipy.stats import mode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background estimation\n",
    "\n",
    "num_images = 200\n",
    "\n",
    "modes = np.zeros(num_images)\n",
    "means = np.zeros(num_images)\n",
    "medians = np.zeros(num_images)\n",
    "\n",
    "for i in range(num_images):\n",
    "    image = skimage.io.imread(f'../data/BBBC039/images_raw/image{i}.tif')\n",
    "    \n",
    "    image = skimage.transform.downscale_local_mean(image, factors = (2,2))\n",
    "    \n",
    "    low = np.percentile(image, 0.01)\n",
    "    high = np.percentile(image, 99.99)\n",
    "\n",
    "    image = np.minimum(image, high)\n",
    "    image = np.maximum(image, low)\n",
    "    \n",
    "    modes[i] = mode(image[image<300]).mode\n",
    "    means[i] = image[image<300].mean()\n",
    "    medians[i] = np.median(image[image<300])\n",
    "\n",
    "fig,ax=plt.subplots(1,3)\n",
    "_ = ax[0].hist(modes.flatten())\n",
    "_ = ax[1].hist(means.flatten())\n",
    "_ = ax[2].hist(medians.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 200\n",
    "dim = 256\n",
    "images = torch.zeros(num_images, dim, dim)\n",
    "\n",
    "for i in range(num_images):\n",
    "    image = skimage.io.imread(f'../data/BBBC039/images_raw/image{i}.tif')\n",
    "    \n",
    "    image = skimage.transform.downscale_local_mean(image, factors = (2,2))\n",
    "    \n",
    "    high = np.percentile(image, 99.99)\n",
    "    image = np.minimum(image, high)\n",
    "\n",
    "    image = image[:dim,:dim]\n",
    "    \n",
    "    images[i] = torch.from_numpy(np.array(image, dtype = np.float64)[:dim,:dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fluorescence empirical distribution\n",
    "\n",
    "background = 160\n",
    "_ = plt.hist((images[images>(2*background)] - background).flatten().cpu().numpy(), bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_counts = torch.zeros(num_images)\n",
    "masks = torch.zeros(num_images, 2*dim, 2*dim)\n",
    "\n",
    "for i in range(num_images):\n",
    "    mask = skimage.io.imread(f'../data/BBBC039/masks_raw/mask{i}.png')\n",
    "    mask = mask[:(2*dim),:(2*dim),0]\n",
    "    masks[i] = torch.tensor(skimage.morphology.label(mask))\n",
    "    true_counts[i] = len(torch.unique(masks[i])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(images, '../data/BBBC039/images.pt')\n",
    "torch.save(masks, '../data/BBBC039/masks.pt')\n",
    "torch.save(true_counts, '../data/BBBC039/true_counts.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
